# Data Lakes With Spark

#### Project Overview:
<p style='text-align: justify;'>
Sparkify is a music streaming application which is being used by millions of people
across the globe to tune in to their favourite music. It generates revenue from the
monthly subscription charges paid by its users. Since the competition in this industry is huge, Sparkify's analytical team wants to take advantage of its data which is being generated every second from its application to make informed decisions. To achieve this, Sparkify employs a data engineer to create a data warehouse (OLAP) which can be used to analyse the data efficiently.
</p>

#### **Data:**
<p style='text-align: justify;'>
The data for this project is located in AWS S3 data lake. There are two sets of data:<br>

1.	songs data - Consists of the information about the songs in the database
2. events data - Consists of the events logs data generated by the user in the application

</p>

#### **Database Schema Design:**
<div style="text-align: justify">
In this project we have gone ahead with a star schema with one fact table and four dimension tables. We have chose star schema because unlike snowflake schema, it is denormalized which makes the analysts use less joins which in turn means higher efficiency of the queries.
</div>
<br>
<div style="text-align: justify">
Songplays(Fact Table): records in log data associated with song plays i.e. records with page 'NextSong'
</div>
<br>
<div style="text-align: justify">
Users(Dim Table): users using Sparkify app <br>
Songs(Dim Table): songs in the Sparkify Database <br>
Artists(Dim Table): Artists in the Sparkify Database <br>
Time(Dim Table): Timestamps of records in 'songplays' broken down to specific units 
</div>

#### **ETL Pipeline Design:**

There are two files which are used for our ETL process:  
1. etl.py
2. dl.cfg

#### **etl.py:**
<div style="text-align: justify">
This is a python script which consists of the PySpark code to process our data which is situated in the S3 location. Once processed and necessary tables are created, it is written in parquet format to S3 location in a separate directory.
This script can be run using spark submit command in the terminal. Best way is to spin up a EMR cluster and we can run this script in our EC2 machine.
</div>

#### **dl.cfg:**
<div style="text-align: justify">
This is a configuration file which consists the AWS credentials which is used to interact with the S3 bucket to read and write data. There is a library called configparser in python to read this file and use the contents to communicate with AWS.
</div>
